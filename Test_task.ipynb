{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOlCZGrnDT+a+T+ju7gf1zB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","# настройки\n","batch_size = 128\n","epochs = 10\n","finetune_epochs = 10\n","lr = 0.01\n","rank_ratio = 0.3\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# данные\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n","])\n","trainset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n","testset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n","trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","testloader  = DataLoader(testset,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n","\n","# модели\n","class TeacherNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n","        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(8*8*128, 512)\n","        self.fc2 = nn.Linear(512, 10)\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 8*8*128)\n","        x = F.relu(self.fc1(x))\n","        return self.fc2(x)\n","\n","class StudentNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(8*8*64, 256)\n","        self.fc2 = nn.Linear(256, 10)\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 8*8*64)\n","        x = F.relu(self.fc1(x))\n","        return self.fc2(x)\n","\n","# utils\n","def train_epoch(model, loader, opt, criterion):\n","    model.train()\n","    total = 0.0\n","    for x,y in tqdm(loader, leave=False):\n","        x,y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n","        opt.zero_grad()\n","        out = model(x)\n","        loss = criterion(out, y)\n","        loss.backward()\n","        opt.step()\n","        total += loss.item()\n","    return total/len(loader)\n","\n","def evaluate(model, loader):\n","    model.eval()\n","    ok, n = 0, 0\n","    with torch.no_grad():\n","        for x,y in loader:\n","            x,y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n","            out = model(x)\n","            ok += (out.argmax(1)==y).sum().item()\n","            n  += y.size(0)\n","    return ok/n\n","\n","def distill_loss(s_out, t_out, y, T=3.0, alpha=0.7):\n","    ce = F.cross_entropy(s_out, y)\n","    kd = F.kl_div(F.log_softmax(s_out/T, dim=1), F.softmax(t_out/T, dim=1), reduction=\"batchmean\")*(T*T)\n","    return alpha*kd + (1-alpha)*ce\n","\n","def count_total_params(m):\n","    return sum(p.numel() for p in m.parameters())\n","\n","def count_linear_params(m):\n","    s = 0\n","    for mod in m.modules():\n","        if isinstance(mod, nn.Linear):\n","            s += sum(p.numel() for p in mod.parameters())\n","    return s\n","\n","# SVD-компрессия (device-safe)\n","def svd_compress_linear(layer: nn.Linear, rank_ratio: float):\n","    W = layer.weight.data\n","    bias = layer.bias.data.clone() if layer.bias is not None else None\n","\n","    U, S, Vh = torch.linalg.svd(W, full_matrices=False)\n","    r = max(1, int(rank_ratio * min(W.shape[0], W.shape[1])))\n","\n","    U_r = U[:, :r]                     # (out, r)\n","    S_r = torch.diag(S[:r])            # (r, r)\n","    Vh_r = Vh[:r, :]                   # (r, in)\n","\n","    # первый: in->r (вес shape=(r,in) == Vh_r)\n","    first  = nn.Linear(W.shape[1], r, bias=False, dtype=W.dtype, device=W.device)\n","    first.weight.data = Vh_r           # (r, in)\n","\n","    # второй: r->out (вес shape=(out,r) == U_r @ S_r)\n","    second = nn.Linear(r, W.shape[0], bias=True, dtype=W.dtype, device=W.device)\n","    second.weight.data = U_r @ S_r\n","    if bias is not None:\n","        second.bias.data = bias\n","    else:\n","        second.bias.data.zero_()\n","\n","    new_seq = nn.Sequential(first, second).to(W.device)\n","    orig_params = W.numel() + (layer.bias.numel() if layer.bias is not None else 0)\n","    new_params  = first.weight.numel() + second.weight.numel() + (second.bias.numel())\n","    return new_seq, orig_params, new_params, r\n","\n","def compress_model_linear_only(model: nn.Module, rank_ratio: float):\n","    stats = []\n","    for name, module in model.named_children():\n","        if isinstance(module, nn.Linear):\n","            new_layer, orig_p, new_p, r = svd_compress_linear(module, rank_ratio)\n","            setattr(model, name, new_layer)\n","            stats.append((name, module.in_features, module.out_features, r, orig_p, new_p))\n","        else:\n","            stats.extend(compress_model_linear_only(module, rank_ratio))\n","    return stats\n","\n","# обучение\n","teacher = TeacherNet().to(device)\n","opt_t = torch.optim.SGD(teacher.parameters(), lr=lr, momentum=0.9)\n","crit = nn.CrossEntropyLoss()\n","\n","print(\"обучаем teacher\")\n","for e in range(epochs):\n","    loss = train_epoch(teacher, trainloader, opt_t, crit)\n","    acc  = evaluate(teacher, testloader)\n","    print(f\"epoch {e+1}/{epochs}, loss={loss:.4f}, acc={acc:.4f}\")\n","teacher_acc    = evaluate(teacher, testloader)\n","teacher_params = count_total_params(teacher)\n","\n","student = StudentNet().to(device)\n","opt_s = torch.optim.SGD(student.parameters(), lr=lr, momentum=0.9)\n","\n","print(\"обучаем student с distillation\")\n","for e in range(epochs):\n","    student.train(); teacher.eval()\n","    total = 0.0\n","    for x,y in tqdm(trainloader, leave=False):\n","        x,y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n","        opt_s.zero_grad()\n","        with torch.no_grad():\n","            t_out = teacher(x)\n","        s_out = student(x)\n","        loss  = distill_loss(s_out, t_out, y, T=3.0, alpha=0.7)\n","        loss.backward()\n","        opt_s.step()\n","        total += loss.item()\n","    acc = evaluate(student, testloader)\n","    print(f\"epoch {e+1}/{epochs}, loss={total/len(trainloader):.4f}, acc={acc:.4f}\")\n","\n","student_acc_before      = evaluate(student, testloader)\n","student_params_before_T = count_total_params(student)\n","student_params_before_L = count_linear_params(student)\n","\n","# компрессия линейных слоёв\n","print(\"сжимаем линейные слои svd с rank_ratio=\", rank_ratio)\n","stats = compress_model_linear_only(student, rank_ratio=rank_ratio)\n","\n","student_params_after_T = count_total_params(student)\n","student_params_after_L = count_linear_params(student)\n","\n","# короткий fine-tune после SVD\n","opt_f = torch.optim.SGD(student.parameters(), lr=lr, momentum=0.9)\n","print(\"finetune student после svd\")\n","for e in range(finetune_epochs):\n","    loss = train_epoch(student, trainloader, opt_f, crit)\n","    acc  = evaluate(student, testloader)\n","    print(f\"finetune epoch {e+1}/{finetune_epochs}, loss={loss:.4f}, acc={acc:.4f}\")\n","\n","student_acc_after = evaluate(student, testloader)\n","\n","# вывод по критерию задачи\n","print(\"\\nподробные слои (только linear):\")\n","for name, inf, outf, r, orig_p, new_p in stats:\n","    ratio = orig_p / new_p if new_p>0 else 0.0\n","    print(f\"layer {name}: in={inf} out={outf} k={r} params {orig_p}->{new_p}, сжатие {ratio:.2f}x\")\n","\n","print(\"\\nитог по линейным слоям:\")\n","print(f\"linear params до: {student_params_before_L}, после: {student_params_after_L}, сжатие: {student_params_before_L/student_params_after_L:.2f}x\")\n","\n","print(\"\\nсравнение моделей:\")\n","print(f\"teacher params (все): {teacher_params}\")\n","print(f\"student params (все) до: {student_params_before_T}, после: {student_params_after_T}\")\n","print(f\"степень сжатия по всем параметрам student: {student_params_before_T/student_params_after_T:.2f}x\")\n","\n","print(\"\\nточность:\")\n","print(f\"точность teacher: {teacher_acc:.4f}\")\n","print(f\"точность student до сжатия: {student_acc_before:.4f}\")\n","print(f\"точность student после сжатия+finetune: {student_acc_after:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_oQLdpxPyoBW","executionInfo":{"status":"ok","timestamp":1756147834915,"user_tz":-240,"elapsed":426364,"user":{"displayName":"Dam Ro Rus","userId":"14760972723641254596"}},"outputId":"507eda5a-ab91-45cd-f4b1-cc5f6f75bd4c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["обучаем teacher\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 1/10, loss=1.6366, acc=0.5442\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 2/10, loss=1.1786, acc=0.6134\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 3/10, loss=0.9720, acc=0.6602\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 4/10, loss=0.8320, acc=0.6837\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 5/10, loss=0.7138, acc=0.7126\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 6/10, loss=0.5960, acc=0.7312\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 7/10, loss=0.4914, acc=0.7310\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 8/10, loss=0.3748, acc=0.7427\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 9/10, loss=0.2718, acc=0.7404\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 10/10, loss=0.1838, acc=0.7425\n","обучаем student с distillation\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 1/10, loss=4.6862, acc=0.5946\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 2/10, loss=2.1613, acc=0.6774\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 3/10, loss=1.4373, acc=0.7112\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 4/10, loss=1.0322, acc=0.7282\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 5/10, loss=0.7648, acc=0.7277\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 6/10, loss=0.5780, acc=0.7388\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 7/10, loss=0.4501, acc=0.7351\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 8/10, loss=0.3611, acc=0.7367\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 9/10, loss=0.3009, acc=0.7377\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["epoch 10/10, loss=0.2557, acc=0.7400\n","сжимаем линейные слои svd с rank_ratio= 0.3\n","finetune student после svd\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["finetune epoch 1/10, loss=0.9227, acc=0.6769\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["finetune epoch 2/10, loss=0.6916, acc=0.6978\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["finetune epoch 3/10, loss=0.5638, acc=0.7045\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["finetune epoch 4/10, loss=0.4764, acc=0.7008\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["finetune epoch 5/10, loss=0.3950, acc=0.7046\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["finetune epoch 6/10, loss=0.3300, acc=0.7005\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["finetune epoch 7/10, loss=0.2786, acc=0.7037\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["finetune epoch 8/10, loss=0.2521, acc=0.7028\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["finetune epoch 9/10, loss=0.2072, acc=0.6891\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["finetune epoch 10/10, loss=0.1827, acc=0.7011\n","\n","подробные слои (только linear):\n","layer fc1: in=4096 out=256 k=76 params 1048832->331008, сжатие 3.17x\n","layer fc2: in=256 out=10 k=3 params 2570->808, сжатие 3.18x\n","\n","итог по линейным слоям:\n","linear params до: 1051402, после: 331816, сжатие: 3.17x\n","\n","сравнение моделей:\n","teacher params (все): 4275594\n","student params (все) до: 1070794, после: 351208\n","степень сжатия по всем параметрам student: 3.05x\n","\n","точность:\n","точность teacher: 0.7425\n","точность student до сжатия: 0.7400\n","точность student после сжатия+finetune: 0.7011\n"]}]}]}